{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import folktables\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from folk_tables_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=12)   \n",
    "plt.rc('ytick', labelsize=12)   \n",
    "plt.rc('legend', fontsize=12)\n",
    "plt.rc('font', family='serif', serif='Palatino')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_folk_cols = [\n",
    "        'AGEP',\n",
    "        'SCHL',\n",
    "        'MAR',\n",
    "        'SEX',\n",
    "        'DIS',\n",
    "        'MIG',\n",
    "        'RELP',\n",
    "        'RAC1P',\n",
    "        'PUMA',\n",
    "        'CIT',\n",
    "        'OCCP',\n",
    "        'JWTR',\n",
    "        'POWPUMA',\n",
    "        'POVPIP',\n",
    "    ]\n",
    "target = 'JWMNP'\n",
    "dummy_cols = [\n",
    "        'MAR',\n",
    "        'SEX',\n",
    "        'DIS',\n",
    "        'ESP',\n",
    "        'MIG',\n",
    "        'RAC1P',\n",
    "        'CIT',\n",
    "        'JWTR',\n",
    "    ]\n",
    "\n",
    "def travel_time_filter(data):\n",
    "    \"\"\"\n",
    "    Filters for the employment prediction task\n",
    "    \"\"\"\n",
    "    df = data\n",
    "    df = df[df['AGEP'] > 16]\n",
    "    df = df[df['PWGTP'] >= 1]\n",
    "    df = df[df['ESR'] == 1]\n",
    "    df = df[df['JWMNP'] >= 0]\n",
    "    return df\n",
    "preprocess=travel_time_filter\n",
    "target_transform= None\n",
    "\n",
    "SubmodelClass = LinearRegressionModel\n",
    "ImputedModelClass = ImputedLinearRegressionModel\n",
    "AggregatorClass = MyModelAggregator\n",
    "\n",
    "ACSIncomeNew = folktables.BasicProblem(\n",
    "    features=all_folk_cols,\n",
    "    target=target,\n",
    "    target_transform=target_transform,    \n",
    "    group='SEX',\n",
    "    preprocess=preprocess,\n",
    "    postprocess=lambda x: np.nan_to_num(x, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_make_projection_permutation_matrix():\n",
    "    data_source = folktables.ACSDataSource(survey_year='2018', horizon='1-Year', survey='person')\n",
    "    acs_data = data_source.get_data(states=[\"AL\"], download=True)\n",
    "    features, label, group = ACSIncomeNew.df_to_pandas(acs_data)\n",
    "    df1 = features\n",
    "    df2 = df1[all_folk_cols[:2]]\n",
    "    df1_dummy = pd.get_dummies(df1, columns= list(set(df1.columns) & set(dummy_cols)) , drop_first=True)\n",
    "    df2_dummy = pd.get_dummies(df2, columns= list(set(df2.columns) & set(dummy_cols)), drop_first=True)\n",
    "    proj_plus, proj_minus = make_projection_permutation_matrix(df1_dummy.columns, df2_dummy.columns)\n",
    "    proj = np.vstack([proj_plus, proj_minus])\n",
    "    assert proj_plus.shape == (len(df2_dummy.columns), len(df1_dummy.columns))\n",
    "    assert proj_minus.shape == ( len(df1_dummy.columns) - len(df2_dummy.columns), len(df1_dummy.columns))\n",
    "    assert np.all(np.sum(proj, axis=0) == 1)\n",
    "    assert np.all(np.sum(proj, axis=1) == 1)\n",
    "    assert np.all(np.logical_or(proj == 0, proj == 1))\n",
    "\n",
    "    a,b = make_projection_permutation(df1_dummy.columns, df2_dummy.columns)\n",
    "    inverse = np.eye(len(df1_dummy.columns))[inverse_perm(a + b)]\n",
    "    assert np.all(inverse @ proj == np.eye(len(df1_dummy.columns)))\n",
    "\n",
    "    print(\"make_projection_permutation_matrix passed!\")\n",
    "\n",
    "test_make_projection_permutation_matrix()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folktables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_datasets = 5\n",
    "year_list = [\"2018\"] * num_datasets\n",
    "states_list = [[\"CA\"], [\"NY\"], [\"TX\"], [\"FL\"], [\"IL\"]]\n",
    "dataset_folk_cols_list = [\n",
    "    all_folk_cols,\n",
    "    all_folk_cols[1:],\n",
    "    all_folk_cols[2:],\n",
    "    all_folk_cols[4:],\n",
    "    all_folk_cols[6:]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_dataset_list = []\n",
    "dataset_list = []\n",
    "og_val_dataset_list = []\n",
    "val_dataset_list = []\n",
    "og_test_dataset_list = []\n",
    "test_dataset_list = []\n",
    "\n",
    "def get_dummy_columns(dummy_cols, folk_cols):\n",
    "    return list(set(dummy_cols) & set(folk_cols))\n",
    "\n",
    "for idx in range(num_datasets):\n",
    "    year = year_list[idx]\n",
    "    states = states_list[idx]\n",
    "    folk_cols = dataset_folk_cols_list[idx]\n",
    "\n",
    "    data_source = folktables.ACSDataSource(survey_year=year, horizon='1-Year', survey='person')\n",
    "    acs_data = data_source.get_data(states=states, download=True)\n",
    "    features, label, group = ACSIncomeNew.df_to_pandas(acs_data)\n",
    "    label = label * 1.0\n",
    "\n",
    "    subfeatures = features[folk_cols]\n",
    "    subfeatures = pd.get_dummies(subfeatures, columns=list(set(dummy_cols) & set(folk_cols)), drop_first=True)\n",
    "    features = pd.get_dummies(features, columns=list(set(dummy_cols) & set(all_folk_cols)), drop_first=True)\n",
    "\n",
    "    features_train, features_test, label_train, label_test = train_test_split(features, label, test_size=0.2, random_state=42)\n",
    "    features_train, features_val, label_train, label_val = train_test_split(features_train, label_train, test_size=0.2, random_state=42)\n",
    "\n",
    "    subfeatures_train, subfeatures_test, sublabel_train, sublabel_test = train_test_split(subfeatures, label, test_size=0.2, random_state=42)\n",
    "    subfeatures_train, subfeatures_val, sublabel_train, sublabel_val = train_test_split(subfeatures_train, sublabel_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    if SubmodelClass != LogisticRegressionModel:\n",
    "        label_train = np.log(label_train)\n",
    "        label_val = np.log(label_val)\n",
    "        label_test = np.log(label_test)\n",
    "        sublabel_train = np.log(sublabel_train)\n",
    "        sublabel_val = np.log(sublabel_val)\n",
    "        sublabel_test = np.log(sublabel_test)\n",
    "\n",
    "    assert np.allclose(label_train, sublabel_train), idx\n",
    "    assert np.allclose(label_val, sublabel_val)\n",
    "    assert np.allclose(label_test, sublabel_test)\n",
    "\n",
    "    og_dataset = FolktablesDataset(name=year, features=features_train, labels=label_train, columns=features_train.columns,transforms=PandasToNumpyTransform())\n",
    "    dataset = FolktablesDataset(name=year, features=subfeatures_train, labels=label_train, columns=subfeatures_train.columns,transforms=PandasToNumpyTransform())\n",
    "\n",
    "    og_val_dataset = FolktablesDataset(name=year, features=features_val, labels=label_val, columns=features_val.columns, transforms=PandasToNumpyTransform())\n",
    "    val_dataset = FolktablesDataset(name=year, features=subfeatures_val, labels=label_val, columns=subfeatures_val.columns, transforms=PandasToNumpyTransform())\n",
    "\n",
    "    og_test_dataset = FolktablesDataset(name=year, features=features_test, labels=label_test, columns=features_test.columns,transforms=PandasToNumpyTransform())\n",
    "    test_dataset = FolktablesDataset(name=year, features=subfeatures_test, labels=label_test, columns=subfeatures_test.columns,transforms=PandasToNumpyTransform())\n",
    "\n",
    "    # center data using training data statistics\n",
    "    def normalize_data(df, mean=None, std=None):\n",
    "        if mean is None:\n",
    "            mean = df.mean()\n",
    "        if std is None:\n",
    "            std = df.std()\n",
    "        return (df - mean) / (std + 1e-7), mean, std\n",
    "    og_dataset.features, features_train_mean, features_train_std = normalize_data(og_dataset.features)\n",
    "    og_features_train_mean = features_train_mean\n",
    "    og_features_train_std = features_train_std\n",
    "    og_val_dataset.features, _, _ = normalize_data(og_val_dataset.features, features_train_mean, features_train_std)\n",
    "    og_test_dataset.features, _, _ = normalize_data(og_test_dataset.features, features_train_mean, features_train_std)\n",
    "    if SubmodelClass != LogisticRegressionModel:\n",
    "        og_dataset.labels, label_train_mean, label_train_std = normalize_data(og_dataset.labels)\n",
    "        og_val_dataset.labels, _, _ = normalize_data(og_val_dataset.labels, label_train_mean, label_train_std)\n",
    "        og_test_dataset.labels, _, _ = normalize_data(og_test_dataset.labels, label_train_mean, label_train_std)\n",
    "\n",
    "    dataset.features, features_train_mean, features_train_std = normalize_data(dataset.features)\n",
    "    val_dataset.features, _, _ = normalize_data(val_dataset.features, features_train_mean, features_train_std)\n",
    "    test_dataset.features, _, _ = normalize_data(test_dataset.features, features_train_mean, features_train_std)\n",
    "    if SubmodelClass != LogisticRegressionModel:\n",
    "        dataset.labels, label_train_mean, label_train_std = normalize_data(dataset.labels)\n",
    "        val_dataset.labels, _, _ = normalize_data(val_dataset.labels, label_train_mean, label_train_std)\n",
    "        test_dataset.labels, _, _ = normalize_data(test_dataset.labels, label_train_mean, label_train_std)\n",
    "\n",
    "    og_dataset_list.append(og_dataset)\n",
    "    dataset_list.append(dataset)\n",
    "    og_val_dataset_list.append(og_val_dataset)\n",
    "    val_dataset_list.append(val_dataset)\n",
    "    og_test_dataset_list.append(og_test_dataset)\n",
    "    test_dataset_list.append(test_dataset)\n",
    "all_og_dataset = og_dataset_list[0]\n",
    "for og_dataset in og_dataset_list[1:]:\n",
    "    all_og_dataset += og_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(len(og_val_dataset)) for og_val_dataset in og_dataset_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[print(len(dataset.columns)) for dataset in dataset_list]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Cov Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigenvalues of covariance matrix\n",
    "cov_matrix, _, _ =compute_covariance_matrix(all_og_dataset)\n",
    "eigvals, eigvecs = np.linalg.eig(cov_matrix)\n",
    "print(np.max(eigvals) / np.min(eigvals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a heatmap of the covariance matrix\n",
    "sns.heatmap(np.abs(cov_matrix))\n",
    "plt.xlabel('Feature Index', fontsize=20)\n",
    "plt.ylabel('Feature Index', fontsize=20)\n",
    "plt.title('Covariance Matrix Heatmap', fontsize=20)\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"gaussian-plots/folktables-covariance-matrix-heatmap.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Base Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itds = dataset_list[-1]\n",
    "gaussian_imse_fn = lambda x, y: -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_list = [200, 300, 400, 500, 600, 700, 800, 900, 1000] +  [2000, 3000, 4000, 5000, 6000, 7000, 8000, 10000]\n",
    "trial_list = np.arange(80)\n",
    "results_list = []\n",
    "result_cols = ['n', 'dataset_idx', 'num_columns', 'mse', 'abse', 'bool_error', 'trial', 'gaussian_imse']\n",
    "models_list = [SubmodelClass() for _ in range(num_datasets)]\n",
    "for n, trial in tqdm(itertools.product(n_list, trial_list), total=len(n_list) * len(trial_list)):\n",
    "    tds = og_test_dataset_list[-1]\n",
    "    # cov_matrix, _, _ = compute_covariance_matrix(all_og_dataset)\n",
    "    imputed_baseline = ImputedModelClass(cov_matrix, og_dataset_list[-1].columns)\n",
    "    rw_imputed_baseline = ImputedModelClass(cov_matrix, og_dataset_list[-1].columns)\n",
    "    \n",
    "    losses_list = []\n",
    "    for i in range(num_datasets):\n",
    "        ds = dataset_list[i]\n",
    "        ds.shuffle(seed=trial)\n",
    "\n",
    "        X, y, columns = ds.get_n_samples_numpy(n)\n",
    "        imputed_baseline.impute(X, y, columns)\n",
    "        models_list[i].fit(X, y, columns)\n",
    "\n",
    "        vds = val_dataset_list[i]\n",
    "        mse = compute_error(vds, model=models_list[i], metric=mse_fn, num_samples=10000, imperfect=True)\n",
    "        losses_list.append(mse)\n",
    "        rw_imputed_baseline.impute(X, y, columns, reweight=1 / mse)\n",
    "\n",
    "        gaussian_imse = compute_error(tds, model=models_list[i], metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "        mse = compute_error(tds, model=models_list[i], metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "        abse = compute_error(tds, model=models_list[i], metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "        bool_error = compute_error(tds, model=models_list[i], metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "        results_list.append((n, str(i), str(len(columns)), mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    agg_model = AggregatorClass()\n",
    "    losses_list = np.array(losses_list)\n",
    "    agg_model.fit(all_columns=og_dataset_list[-1].columns, model_list=models_list, cov_matrix=cov_matrix, losses_list=losses_list)\n",
    "    gaussian_imse = compute_error(tds, model=agg_model, metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "    mse = compute_error(tds, model=agg_model, metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "    abse = compute_error(tds, model=agg_model, metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "    bool_error = compute_error(tds, model=agg_model, metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "    results_list.append((n, \"agg\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    myds = og_dataset_list[-1]\n",
    "    myds.shuffle(seed=trial)\n",
    "    myfeatures, mylabels, mycolumns = myds.get_n_samples_numpy(n)\n",
    "\n",
    "    # identity_cov_ds = GaussianDataset(theta=tds.theta, cov=np.eye(tds.theta.shape[0]), sigma=0.0, columns=tds.columns)\n",
    "    naive_agg_model = NaiveAggregator(all_columns=og_dataset_list[-1].columns, model_list=models_list)\n",
    "    gaussian_imse = -1\n",
    "    mse = compute_error(tds, model=naive_agg_model, metric=mse_fn, num_samples=6000, imperfect=True)\n",
    "    abse = compute_error(tds, model=naive_agg_model, metric=abse_fn, num_samples=6000, imperfect=True)\n",
    "    bool_error = compute_error(tds, model=naive_agg_model, metric=boolerr_fn, num_samples=6000, imperfect=True)\n",
    "    #parameter error option\n",
    "    # mse = compute_error(identity_cov_ds, model=naive_agg_model, metric=mse_fn, num_samples=6000, imperfect=True)\n",
    "    # abse = compute_error(identity_cov_ds, model=naive_agg_model, metric=abse_fn, num_samples=6000, imperfect=True)\n",
    "    # bool_error = compute_error(identity_cov_ds, model=naive_agg_model, metric=boolerr_fn, num_samples=6000, imperfect=True)\n",
    "    results_list.append((n, \"naive_agg\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    naive_agg_model.fit(myfeatures, mylabels, X_columns=mycolumns)\n",
    "    gaussian_imse = -1\n",
    "    mse = compute_error(tds, model=naive_agg_model, metric=mse_fn, num_samples=6000, imperfect=True)\n",
    "    abse = compute_error(tds, model=naive_agg_model, metric=abse_fn, num_samples=6000, imperfect=True)\n",
    "    bool_error = compute_error(tds, model=naive_agg_model, metric=boolerr_fn, num_samples=6000, imperfect=True)\n",
    "    #parameter error option\n",
    "    # mse = compute_error(identity_cov_ds, model=naive_agg_model, metric=mse_fn, num_samples=6000, imperfect=True)\n",
    "    # abse = compute_error(identity_cov_ds, model=naive_agg_model, metric=abse_fn, num_samples=6000, imperfect=True)\n",
    "    # bool_error = compute_error(identity_cov_ds, model=naive_agg_model, metric=boolerr_fn, num_samples=6000, imperfect=True)\n",
    "    results_list.append((n, \"opt_naive_agg\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    imputed_baseline.fit()\n",
    "    gaussian_imse = compute_error(tds, model=imputed_baseline, metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "    mse = compute_error(tds, model=imputed_baseline, metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "    abse = compute_error(tds, model=imputed_baseline, metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "    bool_error = compute_error(tds, model=imputed_baseline, metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "    results_list.append((n, \"imputed_baseline\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    rw_imputed_baseline.fit()\n",
    "    gaussian_imse = compute_error(tds, model=rw_imputed_baseline, metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "    mse = compute_error(tds, model=rw_imputed_baseline, metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "    abse = compute_error(tds, model=rw_imputed_baseline, metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "    bool_error = compute_error(tds, model=rw_imputed_baseline, metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "    results_list.append((n, \"rw_imputed_baseline\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    # HACK to get the right number of columns ######\n",
    "    myds = itds\n",
    "    # myds = dataset_list[0]\n",
    "    # myds = ub_lb_dataset\n",
    "    myds.shuffle(seed=trial)\n",
    "    myfeatures, mylabels, mycolumns = myds.get_n_samples_numpy(n)\n",
    "    # ######\n",
    "\n",
    "    baseline_model = SubmodelClass()\n",
    "    baseline_model.fit(myfeatures, mylabels, mycolumns)\n",
    "    gaussian_imse = compute_error(tds, model=baseline_model, metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "    mse = compute_error(tds, model=baseline_model, metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "    abse = compute_error(tds, model=baseline_model, metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "    bool_error = compute_error(tds, model=baseline_model, metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "    results_list.append((n, \"lb_baseline\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n",
    "\n",
    "    myfeatures, mylabels, mycolumns = myds.get_n_samples_numpy(num_datasets * n)\n",
    "    baseline_model = SubmodelClass()\n",
    "    baseline_model.fit(myfeatures, mylabels, mycolumns)\n",
    "    gaussian_imse = compute_error(tds, model=baseline_model, metric=gaussian_imse_fn, num_samples=6000, imperfect=False)\n",
    "    mse = compute_error(tds, model=baseline_model, metric=hacky_cov_mse_fn, num_samples=6000, imperfect=False)\n",
    "    abse = compute_error(tds, model=baseline_model, metric=abse_fn, num_samples=6000, imperfect=False)\n",
    "    bool_error = compute_error(tds, model=baseline_model, metric=boolerr_fn, num_samples=6000, imperfect=False)\n",
    "    results_list.append((n, \"ub_baseline\", \"NA\", mse, abse, bool_error, trial, gaussian_imse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list, columns=result_cols)\n",
    "og_results_df = results_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR load from file\n",
    "og_results_df= pd.read_csv(\"folktables-results-05-12-23-useILfortest-fixeditds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = og_results_df.copy()\n",
    "method_name_remap = {\n",
    "    \"agg\": \"Collab\",\n",
    "    \"rw_imputed_baseline\": \"RW-Imputation\",\n",
    "    \"imputed_baseline\": \"Imputation\",\n",
    "    \"naive_agg\": \"Naive-Collab\",\n",
    "    \"opt_naive_agg\": \"Optimized-Naive-Collab\",\n",
    "    \"lb_baseline\": \"Naive-Local\",\n",
    "    \"ub_baseline\": f\"Naive-Local ({num_datasets}x data)\"\n",
    "}\n",
    "results_df = results_df[results_df[\"dataset_idx\"].isin(method_name_remap.keys())]\n",
    "results_df['dataset_idx'] = results_df['dataset_idx'].map(method_name_remap)\n",
    "\n",
    "unique_dataset_idx = results_df['dataset_idx'].unique()\n",
    "\n",
    "# Create a color palette and a marker list\n",
    "palette = dict(zip(unique_dataset_idx, sns.color_palette(n_colors=len(unique_dataset_idx))))\n",
    "markers = dict(zip(unique_dataset_idx, ['o', 'v', '^', '<', '>', '8', 's', 'p', '*', 'h', 'H', 'D', 'd', 'P', 'X'][:len(unique_dataset_idx)]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Folktables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_methods_list = [\"Collab\", \"RW-Imputation\", \"Imputation\", \"Naive-Collab\", \"Optimized-Naive-Collab\", \"Naive-Local\", f\"Naive-Local ({num_datasets}x data)\"]\n",
    "plotdf = results_df[results_df['dataset_idx'].isin(plot_methods_list)]\n",
    "# select n greater than 1000\n",
    "plotdf = plotdf[plotdf['n'] >= 2000]\n",
    "plotdf = plotdf[plotdf['n'] <= 8000]\n",
    "\n",
    "# log scale mse column in plotdf\n",
    "# plotdf['mse'] = np.log10(plotdf['mse'])\n",
    "ax=sns.lineplot(x=\"n\", y=\"mse\", hue=\"dataset_idx\", data=plotdf, style=\"dataset_idx\", markersize=7, palette=palette, markers=markers)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:])\n",
    "plt.ylabel(\"Mean Sq. Pred. Err.\", fontsize=20)\n",
    "plt.xlabel(\"number of samples (n)\", fontsize=20)\n",
    "plt.title(\"Illinois Full Feature Prediction Error (Large n)\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "#log scale y\n",
    "# plt.yscale('log')\n",
    "\n",
    "# #set xlim\n",
    "# plt.xlim(1000, 8000)\n",
    "plt.ylim(0.85, 0.95)\n",
    "\n",
    "# plt.savefig(\"gaussian-plots/folktables-full-prediction-states-large-n-v3.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_methods_list = [\"Collab\", \"RW-Imputation\", \"Imputation\", \"Naive-Collab\", \"Optimized-Naive-Collab\",  f\"Naive-Local ({num_datasets}x data)\"]\n",
    "plotdf = results_df[results_df['dataset_idx'].isin(plot_methods_list)]\n",
    "# select n greater than 1000\n",
    "plotdf = plotdf[plotdf['n'] < 1000]\n",
    "plotdf = plotdf[plotdf['n'] >= 400]\n",
    "\n",
    "# log scale mse column in plotdf\n",
    "# plotdf['mse'] = np.log10(plotdf['mse'])\n",
    "ax=sns.lineplot(x=\"n\", y=\"mse\", hue=\"dataset_idx\", data=plotdf, style=\"dataset_idx\", markersize=7, palette=palette, markers=markers)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:])\n",
    "plt.ylabel(\"Mean Sq. Pred. Err.\", fontsize=20)\n",
    "plt.xlabel(\"number of samples (n)\", fontsize=20)\n",
    "plt.title(\"Illinois Full Feature Prediction Error (Small n)\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# #set xlim\n",
    "# plt.xlim(1000, 8000)\n",
    "# plt.ylim(0.85, 1.05)\n",
    "\n",
    "# plt.savefig(\"gaussian-plots/folktables-full-prediction-states-small-n-v4.pdf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_methods_list = [\"Collab\",\"Naive-Collab\", \"Optimized-Naive-Collab\",  f\"Naive-Local ({num_datasets}x data)\"]\n",
    "plotdf = results_df[results_df['dataset_idx'].isin(plot_methods_list)]\n",
    "# select n greater than 1000\n",
    "plotdf = plotdf[plotdf['n'] < 1000]\n",
    "plotdf = plotdf[plotdf['n'] >= 400]\n",
    "\n",
    "# log scale mse column in plotdf\n",
    "# plotdf['mse'] = np.log10(plotdf['mse'])\n",
    "ax=sns.lineplot(x=\"n\", y=\"mse\", hue=\"dataset_idx\", data=plotdf, style=\"dataset_idx\", markersize=7, palette=palette, markers=markers)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles=handles[0:], labels=labels[0:])\n",
    "plt.ylabel(\"Mean Sq. Pred. Err.\", fontsize=20)\n",
    "plt.xlabel(\"number of samples (n)\", fontsize=20)\n",
    "plt.title(\"Illinois Full Feature Prediction Error (Small n)\", fontsize=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# #set xlim\n",
    "# plt.xlim(1000, 8000)\n",
    "# plt.ylim(0.85, 1.05)\n",
    "\n",
    "# plt.savefig(\"gaussian-plots/folktables-full-prediction-states-small-n-v4.pdf\")\n",
    "# plt.savefig(\"gaussian-plots/folktables-full-prediction-states-small-n-v5.pdf\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_agg_cpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
